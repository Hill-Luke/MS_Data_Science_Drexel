{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNRqVPXOmzEO"
   },
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 7: Building and Maintaining a Robust Acquisition Stream\n",
    "\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULX1EvLKmzEP"
   },
   "source": [
    "#### 7.1.1.2 Exercise: Understanding API rate limits\n",
    "Read each of the above API docs and describe the how much API usage is allowed per day from each platform for a given app. Do all apps get the same bandwidth? What methods/metrics do the platforms use to determine limits and overuse? How should an app be constructed to maximize data access?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaMx5CNjmzES"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHP8QWO2mzET"
   },
   "source": [
    "#### 7.1.2.3 Exercise: robots.txt\n",
    "Take a look at the robots file for each of `facebook.com` and `amazon.com`. Determine and discuss any allowances/disallowances for bots that you might create to crawl these sites. Do you infer any cultural differences around data sharing and access between these companys and also with Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1xlmP51mzEV"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxvkFnCOmzEX"
   },
   "source": [
    "#### 7.2.1.2 Exercise: understanding a crontab for a recurrent, whole-site data access application\n",
    "Gutenberg is an open data repository, so we should be able to download all of its data!  To start, let's review the robots file on Project Gutenberg's website:\n",
    "- http://www.gutenberg.org/robots.txt\n",
    "\n",
    "What do you notice about this file. Is anyone allowed to crawl the site? Do you think Gutenberg uses the newer, big tech rules? How frequently can we make requests?\n",
    "\n",
    "Use the `robotexclusionrulesparser` module from Section 7.1.2.4 to determine if we can access a given data file. Use the URL for the text copy of Moby dick: \n",
    "- https://www.gutenberg.org/files/2701/2701-0.txt\n",
    "\n",
    "Following the above, review the instructions on mirroring the repository:\n",
    "- https://www.gutenberg.org/wiki/Gutenberg:Mirroring_How-To\n",
    "\n",
    "and explain why Gutenberg requests using the `rsync` command-line utility to copy its data. Can you decode the two presented crontab patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7hmqqpwmzEZ"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o_b1oiAmzEd"
   },
   "outputs": [],
   "source": [
    "## place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU9pvE-bmzEl"
   },
   "source": [
    "#### 7.3.2.2 Exercise: a script restarter using psutil that also kills zombies\n",
    "Rewrite `check_process(name)` above by using psutil to 1) obtain process names more easily without regex, and use this 2) to restart our dummy process if it's finished after 3 or fewer passes in the while loop, and kill it if it's still running after 4 or more passes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdPCnXBwmzEm"
   },
   "outputs": [],
   "source": [
    "## place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENjQ6hwXmzEs"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Setting up an allowable-paths spider \n",
    "Here, the overall goal will be to build a tool that uses the `robotexclusionrulesparser` module to identify all allowable next steps in a web-crawl (using beautiful soup to parse the page).\n",
    "#### 1. Collecting lists of allowed links\n",
    "To start, set up a function that\n",
    "1. collects the robots.txt file for a given url\n",
    "2. constructs all allowable paths granted to 'User-agent: *' specifed by the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0_Zg9ab8VD4"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKnvWUwXmzE6"
   },
   "source": [
    "#### 2. Determine which page links are allowed\n",
    "Using the `robots.txt` file obtained in __(1)__, iterate over the specified site's hyperlinks (using beautiful soup) and output a dictionary of boolean values (keyed by the site's hyperlinks) that indicates which hyperlinks are robotically accessible.\n",
    "\n",
    "__Bonus:__ Discuss the potential usage of any available site maps (what are these?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52d4Vi878O8L"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3rxMcWKmzFF"
   },
   "source": [
    "#### 3. Plan a strategy\n",
    "How will you approach a broader site crawl? Specifically, determine a strategy for going deep into the site, repeatedly determineing any new paths and whether or not they are accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUKeoWHemzFG"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdmacyogmzFG"
   },
   "source": [
    "### B. Non-authenticated Twitter API access (shh)\n",
    "Technically, this week's chapter shows us a nice little secret about Twitter's API&mdash;the `robots.txt` file technically shows us we're allowed to pull data from the standard REST search API from their front end. We'll build this up in pieces, first analyzing from our work with Robots.\n",
    "#### 1. Scoping Twitter's crawlable content\n",
    "To start, apply the two tools developed in __(A1&ndash;A2)__ to `twitter.com` to determine what's available from the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqszocRTmzFH"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDi9hWVomzFI"
   },
   "source": [
    "#### 2. Reviewing all allowable links\n",
    "Apply the all allowable links function from __(1)__ to `twitter.com`, and review. In particular focus on the one related to search. Discuss these urls and what they actually provide access to.\n",
    "\n",
    "[__Hint__. Go back to the example in __Sec. 7.1.2.4__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkgsrzm-mzFK"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CclZJuHI24oo"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qFDpayemzFQ"
   },
   "source": [
    "#### 3. Scoping utilility with standard search parameters\n",
    "Review the standard search parameters and attempt to pass some to confirm that we are indeed accessing the same resources (just reviewing them wrapped in html). \n",
    "\n",
    "- https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "\n",
    "In particular, which parameters give us generalized access to the search API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9EM6JhomzFR"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac6u4SlE1VMv"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLx0oJ1wmzFZ"
   },
   "source": [
    "#### 4. Rigorous resource confirmation\n",
    "Since we know the real-time search API is not available to robots (again, go back to  __Sec. 7.1.2.4__), we should probably assume that these robot-available data are not technically the same sample of tweets we'd get from authenticated access. But is this true?\n",
    "\n",
    "Here, the job is to replicate exactx-same API calls between the front-end scraper and Twitter's API to confirm that we're indeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azTAxbkZmzFa"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DymP01853H48"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXHTnyZHmzFk"
   },
   "source": [
    "#### 5. Abstracting the resource\n",
    "What we'd really like to have out of this is a second, unauthenticated Twitter search API that is based off of their front end. But this means that we want 'tweet objects' (see Twttier: https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object). \n",
    "\n",
    "So, here the job is to build a function that works with BeautifulSoup to extract all content from a front-end search result (499 `q` query characters, but otherwise accepts other standard search parameters, as possible) and constructs something as close as possible to  a list of tweet objects with cursors for any available pagenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-hHjmxGmzFn"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WplaYvP-3NSC"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsqNXma24qKX"
   },
   "source": [
    "### C. Non-authenticated Twitter API access (again, shh)\n",
    "Considering the outcome of Exercise B (see Solutions, Section B., above), it seems that if non-authenticated access is possible, that it would be along a different endpoint. In fact, one such endpoint appears to exist because this as already been built. \n",
    "\n",
    "Consider the following Python module, `GetOldTweets3`:\n",
    "\n",
    "- https://github.com/Mottl/GetOldTweets3/blob/master/bin/GetOldTweets3\n",
    "\n",
    "####1. What's gonig on here?\n",
    "Review the module's code on github, particularly the README to assess what it does and why it works. In addition, see if you can get it working in the notebook here. How can you access the code, via github, pip? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYEqjr9Z5wtD"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGJtLBeN8Hqk"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C33KJSo50KB"
   },
   "source": [
    "#### 2. Try out the Module\n",
    "Now let's try out some of the examples on their README to see everything works ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TtBGHIb8Ca_"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN1uvpP_6OrT"
   },
   "source": [
    "#### 3. Evaluate the Level of Access\n",
    "Select an endpoint that this 'API' appears to mimic with respect to Twitter's REST API and compare the volume of requests (as determined by the endpoint compared) that `GetOldTweets3` appears to provide, as compared to the REST API expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbAan39c6poQ"
   },
   "source": [
    "_Discussion._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXtUhoS56ouQ"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XvVrnTP6sCE"
   },
   "source": [
    "#### 4. Explain this Utility\n",
    "Review Twitter's `robots.txt` and determine the best possible explanation you can for why this module is able to exist. In this discussion, include an assessment of its use as a data engineering resource and the potential for its continued existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bXTkD9a7Evw"
   },
   "source": [
    "_Discussion._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BC6uY807DYM"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
