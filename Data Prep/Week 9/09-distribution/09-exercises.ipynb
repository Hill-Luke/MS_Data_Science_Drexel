{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXdvvep3n8GN"
   },
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 9: Distribution, accessibility, and data sharing\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54y4P3-_stsP"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Releasing a fact-checked tweets dataset\n",
    "Previously in Chapter 5, we explored a web scraping task focused on the individuals who have been covered by the fact-checking website, PolitiFact:\n",
    "\n",
    "- https://www.politifact.com/\n",
    "\n",
    "As discussed at that time, while it appears possible to work with their data from the front end, reproducing it or releasing its content is not allowed. So another distribution method for any of its data is required, which we'll explore here.\n",
    "\n",
    "For this exercise, our high-level goal will be to distribute a PolitiFact dataset that could support downstream algorithms that automatically fact-check Tweets. \n",
    "\n",
    "#### 1. Understanding the release data\n",
    "Review the following object in the local data directory:\n",
    "\n",
    "- `./data/fact-checked-tweets.json`\n",
    "\n",
    "and discuss in the response box below what these data describe with respect to the two platforms (PolitiFact and Twitter) and why they are reasonable to release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXO_2EztIHin"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4Ntal82IGh7"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhMbvDKDIMdS"
   },
   "source": [
    "#### 2. Planning hydrated data objects\n",
    "Assuming we'll release the `fact-checked-tweets.json` data, we now need to plan some data downloaders for the PolitiFact and Twitter data that our code release will access and integrate.\n",
    "\n",
    "In particular, discuss what tools we'll have to use in conjunction with `fact-checked-tweets.json` to access the following data:\n",
    "1. fullly hydrated tweet objects\n",
    "2. PolitiFact ratings and fact-check sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRP1_HCWJJzA"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlnbY8b5Ja-e"
   },
   "source": [
    "#### 3. Build a tweet hydrator\n",
    "This is more or less a standard `Twython` task, aside from the fact that the tweet ids themselves will need to be isolated from the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPhLqWPJtcH"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbOF4GmVJt1n"
   },
   "source": [
    "#### 4. Build a ratings scraper\n",
    "The other side of this integration job must now take the specified endpoints and append them onto the following base url:\n",
    "\n",
    "- `https://www.politifact.com`\n",
    "\n",
    "to resolve the appropriate fact-check data. Utilizing `BeautifulSoup`, build a function that constructs a full url, accesses the webpage's html, and applies `BeautifulSoup` to access a given fact-check's rating (from `'True'` to `'Pants on Fire'`) and sources (a list of urls provided at the bottom of the page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y33XS0xjMO2r"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMj4ZEEOMPvt"
   },
   "source": [
    "#### 5. Build a full data access wrapper\n",
    "Now that you have the two data access functions in place, package them up in a script that when run will integrate the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvNMpCI5MnoA"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcocQlXOM2oj"
   },
   "source": [
    "#### 6. Reviewing our withheld code and planning a strategic release\n",
    "While we'll release the `'fact-checked-tweets.json'` data and our script to integrate the full dataset, there are some aspects of our code that had to be constructed which we won't release publicly. \n",
    "\n",
    "To complete this exercise, review the code below to understand how `'fact-checked-tweets.json'` object was constructed and how it could be expanded to full, PolitiFact-wide data set with many more tweets. This discussion should be placed in the response box, below, and specifically discuss how the three main code blocks provided can be utilized together to produce the full data set that we'd like to release.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZp_znMlOBGv"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iengy40UQCZs"
   },
   "outputs": [],
   "source": [
    "## code block 1: accessing a list of all personalities covered by PolitiFact\n",
    "## note: this was not used for the fact-checked tweets, but will be essential\n",
    "## to gather _all_ fact-checked tweets from the site.\n",
    "\n",
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "personalities_url = \"https://www.politifact.com/personalities/\"\n",
    "personalities_html = requests.get(personalities_url).text\n",
    "personalities_soup = BeautifulSoup(personalities_html, 'html.parser')\n",
    "\n",
    "data = {}\n",
    "for i, section in enumerate(personalities_soup.find_all(\"section\", {\"class\": \"o-platform o-platform--has-thin-border o-platform--is-wide\"})):  \n",
    "  title = section.find(\"h2\", {\"class\": \"c-title c-title--section\"}).text.strip()\n",
    "  section_id = section['id']\n",
    "  data[section_id] = {\"id\": section_id, \"section\": title, \"personalities\": {}}\n",
    "  for personality in section.find_all('div', {\"class\": \"c-chyron\"}):\n",
    "    link = personality.find('a')\n",
    "    url = link['href']\n",
    "    group = personality.find('div', {\"class\": \"c-chyron__subline\"}).text.strip()\n",
    "    name = link.text.strip()\n",
    "    personality_id = re.split(\"/\", url)[-2]\n",
    "    data[section_id]['personalities'][personality_id] = {'url': url, \"name\": name, \"id\": personality_id, \"group\": group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V_HHHN_QUGV"
   },
   "outputs": [],
   "source": [
    "## code block 2: accessing a list of all fact checked statements made by a personality\n",
    "## note: the personality 'tweets' is just a catch-all category that the site uses for \n",
    "## fact checks of tweets made by lesser-known personalities (so it appears)\n",
    "\n",
    "list_base_url = \"https://www.politifact.com/factchecks/list/\"\n",
    "for personality in [\"tweets\"]:\n",
    "  next_page = \"?speaker=\" + personality\n",
    "  statements_urls = []\n",
    "  while next_page:\n",
    "    speakerchecks_url = list_base_url + next_page\n",
    "    speakerchecks_html = requests.get(speakerchecks_url).text\n",
    "    speakerchecks_soup = BeautifulSoup(speakerchecks_html, 'html.parser')\n",
    "    for statement in speakerchecks_soup.find_all('li', {'class': 'o-listicle__item'}):\n",
    "      description = statement.find('div', {'class': \"m-statement__desc\"})\n",
    "      if description:\n",
    "        if re.search(\"(tweet|twitter|post)\", description.text.lower()):\n",
    "          statement_url = statement.find('div', {\"class\": \"m-statement__quote\"}).find('a')['href']\n",
    "          statements_urls.append(statement_url)\n",
    "    \n",
    "    next_page = ''\n",
    "    for link in speakerchecks_soup.find_all('a', {\"class\": \"c-button c-button--hollow\"}):\n",
    "      if link.get('href',''):\n",
    "        if link.text ==\"Next\":\n",
    "          next_page = link['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyplXZX6QWio"
   },
   "outputs": [],
   "source": [
    "## code block 3: retrieve any twitter-specific urls of tweets from a each statement's page\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "statement_tweets = defaultdict(list)\n",
    "for statement_url in statements_urls:\n",
    "  statement_html = requests.get(\"https://www.politifact.com\" + statement_url).text\n",
    "  statement_soup = BeautifulSoup(statement_html, 'html.parser')\n",
    "\n",
    "  for link in statement_soup.find('article', {\"class\": \"m-textblock\"}).find_all('a'):\n",
    "    if re.search(\"^https://twitter.com[^ ]+/status/\\d+$\", link.get('href', '')):\n",
    "      statement_tweets[statement_url].append(link['href'])\n",
    "\n",
    "with open('./data/fact-checked-tweets.json', 'w') as f:\n",
    "  f.write(json.dumps(statement_tweets))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09-exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
