{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 2\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 3\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Group 3\n",
    "## Module C _(45 points)_\n",
    "\n",
    "In this assignment, we'll build and expand on an exercise which focuses on a targeted scrape of Wikipedia, extracting content pertaining to the UK cities.\n",
    "\n",
    "Conviently, the individual articles pertaining to the cities of the UK are formed in a list on English Wikipedia:\n",
    "- https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C1.__ _(5 pts)_ First, complete the function to operate the Wikipedia REST API's [documentation here](https://en.wikipedia.org/api/rest_v1/#/) to determine an acceptable endpoint for the html content. Then, use the `requests` module to access the provided list-page. This function should then return the parsed HTML, i.e., using `BeautifulSoup` as `page_soup`, as well as the entire `page_response` from the `requests` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1:Function(5/5)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_page_and_parse(page_title):\n",
    "\n",
    "    API_path = \"https://en.wikipedia.org/api/rest_v1/\"\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "\n",
    "    return page_soup, page_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "'List of cities in the United Kingdom'    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1:SanityCheck\n",
    "\n",
    "cities_page_soup, cities_page_response = get_page_and_parse(\"List_of_cities_in_the_United_Kingdom\")\n",
    "cities_page_soup.find('head').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ _(7 pts)_ Next, your job is to extract the links for the different cities by locating the first object in the html tagged as `'table'`. Looping over the table's (non-header) rows, obtain the first `'a'`-tag (hyperlink) from the first column of each row, and append these links within `city_links`, using the form `[show_text, hyperlink]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:Function(7/7)\n",
    "\n",
    "def extract_city_links(cities_page_soup):\n",
    "    city_links = []\n",
    "    \n",
    "    #---your code starts here---\n",
    "        \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return city_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "[['Aberdeen', './Aberdeen'],\n",
    " ['Armagh', './Armagh'],\n",
    " ['Bangor', './Bangor,_Gwynedd'],\n",
    " ['Bath', './Bath,_Somerset'],\n",
    " ['Belfast', './Belfast'],\n",
    " ['Birmingham', './Birmingham'],\n",
    " ['Bradford', './City_of_Bradford'],\n",
    " ['Brighton & Hove', './Brighton_and_Hove'],\n",
    " ['Bristol', './Bristol'],\n",
    " ['Cambridge', './Cambridge']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "city_links = extract_city_links(cities_page_soup)\n",
    "city_links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3.__ _(7 pts)_ Next, complete the initial data collection task by filling the `city_data` object in the function, below. Each link in `city_links` should utilize the schema:\n",
    "```\n",
    "city_data = {\n",
    "    page_id: {\n",
    "        \"name\": <name of page>,\n",
    "        \"text\": <full html string for page>\n",
    "    }\n",
    "}\n",
    "```\n",
    "where `page_id` now ocrresponds to the `id` of the page used when accessing the API, which in __Part C2__'s output should to correspond to the show texts for each link in `city_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3:Function(7/7)\n",
    "\n",
    "def get_city_data(city_links, cities_page_response):\n",
    "    city_data = {\"./List_of_cities_in_the_United_Kingdom\": {\n",
    "                     \"name\": \"List of cities in the United Kingdom\",\n",
    "                     \"text\": cities_page_response.text}}\n",
    "    API_path = \"https://en.wikipedia.org/api/rest_v1/\"\n",
    "        \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "('Armagh',\n",
    " '<!DOCTYPE html>\\n<html prefix=\"dc: http://purl.org/dc/terms/ mw: http://mediawiki.org/rdf/\" about=\"https://en.wikipedia.org/wiki/Special:Redirect/revision/1048196172\"><head prefix=\"mwr: https://en.wikipedia.org/wiki/Special:Redirect/\"><meta property=\"mw:TimeUuid\" content=\"1df726d0-2b99-11ec-856e-55091143e768\"/><meta charset=\"utf-8\"/><meta property=\"mw:pageId\" content=\"473800\"/><meta property=\"mw:pageNamespace\" content=\"0\"/><link rel=\"dc:replaces\" resource=\"mwr:revision/1047848140\"/><meta property=\"mw:revisionSHA1\" content=\"e4e8a119952ba8c9225de83cb9e025a14f3608d2\"/><meta property=\"dc:modified\" content=\"2021-10-04T19:12:15.000Z\"/><meta property=\"mw:htmlVersion\" content=\"2.3.0\"/><meta property=\"mw:html:version\" content=\"2.3.0\"/><link rel=\"dc:isVersionOf\" href=\"//en.wikipedia.org/wiki/Armagh\"/><title>Armagh</title><base href=\"//en.wikipedia.org/wiki/\"/><meta property=\"mw:styleModules\" content=\"mediawiki.page.gallery.styles|ext.cite.style|ext.cite.styles\"/><link rel=\"stylesheet\" href=\"/w/lo')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3:SanityCheck\n",
    "import json, os\n",
    "\n",
    "# gather the data\n",
    "if os.path.exists(\"./data/city_data.json\"):\n",
    "    city_data = json.load(open(\"./data/city_data.json\"))\n",
    "else:\n",
    "    city_data = get_city_data(city_links, cities_page_response)\n",
    "    # save the data to disk\n",
    "    with open(\"./data/city_data.json\", \"w\") as f:\n",
    "        f.write(json.dumps(city_data))\n",
    "\n",
    "# generate some output\n",
    "(lambda x: (x['name'], x['text'][:1000]))(city_data[list(city_data.keys())[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ _(6 pts)_ Now that we've accessed the primary data, let's reivew the two ways we could spread out to gather more content relating to the cities. On Wikipedia, pages are hyper-linked into a network. This means pages both link to each city (in-links) and out from each city (out-links).\n",
    "\n",
    "To see a list of in-links, i.e., what pages link to a given page, we can use the time-honored 'what links here' endpoint:\n",
    "- `'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere'`\n",
    "\n",
    "Using this, any page can be checked for its in-links, i.e., pages that link to it. \n",
    "\n",
    "Here, your job is to complete the function below, whose plan includes using the specified `params` to ensure that 500 pages are returned per request, and that all pages returned have `namespace=0`, i.e., are core content articles on Wikipedia. Note: the `params` must be concatenated onto the `base_URL`, and then the page's searchable name, `page_id` (using underscores and not spaces), must be concatenated onto that.\n",
    "\n",
    "Once you've parsed the page's `inlinks_soup` object, use `BeautifulSoup` commands to collect the hyperlinks within __all `'li'`-tagged objects that have a `'span'`-tagged object within them, but which _don't_ have a `'class'` attribute.__ This restriction will help us avoide the 'edit' links. For output, each link should be appended as a `(show_text, URL)`-tuple into the `links` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:Function(6/6)\n",
    "\n",
    "def index_inlinks(page_id):\n",
    "    base_URL = 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere'\n",
    "    params = '&limit=500&namespace=0&target='\n",
    "    \n",
    "    links = [] # collect the in-links within this list object\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #--- your code stops here---\n",
    "    \n",
    "    return links, inlinks_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(500,\n",
    " [('Foreign relations of Angola', '/wiki/Foreign_relations_of_Angola'),\n",
    "  ('American Revolutionary War', '/wiki/American_Revolutionary_War'),\n",
    "  ('Ankara', '/wiki/Ankara'),\n",
    "  ('Alfred Hitchcock', '/wiki/Alfred_Hitchcock'),\n",
    "  ('Amsterdam', '/wiki/Amsterdam'),\n",
    "  ('Albert Speer', '/wiki/Albert_Speer'),\n",
    "  ('Foreign relations of Azerbaijan', '/wiki/Foreign_relations_of_Azerbaijan'),\n",
    "  ('Afro Celt Sound System', '/wiki/Afro_Celt_Sound_System'),\n",
    "  ('Alternate history', '/wiki/Alternate_history'),\n",
    "  ('Athens', '/wiki/Athens')])\n",
    "```\n",
    "\n",
    "Note: `'/City_of_London'` is actually our entry of interest for the original cities list, potentially making `'./London'` a poor unit test. Nevertheless, all pages should produce the same output structure, since the inlinks pages are constructed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:SanityCheck\n",
    "\n",
    "links, inlinks_soup = index_inlinks('London')\n",
    "len(links), links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C5.__ _(4 pts)_ Finally, make sure to collect the `'next 500'` pagenation, and to store this as `next_page_URL` by completing the function, below. We'll be able to use this URL to come back for the remainder of the page's list if/when we're ready. [Hint: Use the string `'next 500'` to identify the pagenation link in a given page's search results.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5:Function(4/4)\n",
    "\n",
    "def get_next_page_URL(inlinks_soup):\n",
    "    \n",
    "    next_page_URL = '' # collect the next page of results' URL, \n",
    "                       # provided more results exist\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return next_page_URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "'/w/index.php?title=Special:WhatLinksHere/London&namespace=0&limit=500&dir=next&offset=17792'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5:SanityCheck\n",
    "\n",
    "next_page_URL = get_next_page_URL(inlinks_soup)\n",
    "next_page_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C6.__ _(5 pts)_ As we can see, it will take multiple calls to the function from part __C5__ to complete the collection of _all_ of each page's in-links. This means that knowing how many pages/in-links exist won't _exactly_ be clear from the start. So as a first pass, let's collect each page's first batch of 500 (or fewer) in-links. In theory, then we could check to see just how many of the city-pages have at least a second page (more than 500 in-links).\n",
    "\n",
    "Specifically, your job in this part of the problem is to store up to 500 of each page's in-links in the `links_index` object, whose schema should conform to the following pattern:\n",
    "```\n",
    "links_index[page_id] = {'name': page_name, 'links': links, \n",
    "                                'next_page': next_page_URL}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6:Function(5/5)\n",
    "\n",
    "def collect_first_inlinks_pages(city_links):\n",
    "    links_index = {}\n",
    "    \n",
    "    #---your code starts here---\n",
    "    \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return links_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output could be:\n",
    "```\n",
    "507\n",
    "```\n",
    "Clearly, some extra links are coming through in the answer key. So even though we are requesting a single batch of data as a structured response, the response we've received when unit testing on `'./London'` is sufficiently different in structure from that of `'./Armagh'`, where it's possible that _more_ hyperlinks could be collected than expected! \n",
    "\n",
    "As it turns out, this difference is the result of inlinks that point _through_ 're-direct' links, e.g., which ultimately point back `'./Armagh'`'s   page, such as `'./City_of_Armagh'`. Can you spot the redirect's on `'./Armagh'`'s   [inlinks page](https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Armagh&namespace=0&limit=500)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6:SanityCheck\n",
    "\n",
    "inlinks_index = collect_first_inlinks_pages(city_links)\n",
    "len(inlinks_index['./Armagh']['links'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C7.__ _(3 pts)_ Now let's take another look at our data and see just how few in-links some cities' pages have. Using the output `inlinks_index`, determine which city had the smallest number of in-links, and record this and its id to complete the print statement in the `Inline()` cell, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C7:Inline(3/3)\n",
    "\n",
    "smallest_num_links = float('Inf')\n",
    "smallest_link_city_id = ''\n",
    "\n",
    "#---your code starts here---\n",
    "        \n",
    "#---your code stops here---\n",
    "\n",
    "print('The least in-linked city was ', smallest_link_city_id, \n",
    "      ' with ', smallest_num_links, ' links.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C8.__ _(6 pts)_ Now that we know how big/small the in-link sets can be, let's see if we can gather all of the out-links. This means we'd like to do the same as in part __C7__, but by gathering all hyperlinks within the city wiki pages that point to other primary encyclopedia wiki content. \n",
    "\n",
    "Ideally, we'd want only those links on the pages to other articles on Wikipedia with `namespace = 0`. However to start, your job is to complete the function below using `BeautifulSoup` operations on each city's html response in `city_data` to collect _all_ hyper-links for each page in `city_data` that have a `'title'` attribute. This attribute should refine the links to inside of Wikipedia (although not necessarily with `namespace = 0`). To complete this filtering of the links, use the argument `{'title': True}` to filter your `.find_all()` for the hyper-links.\n",
    "\n",
    "Note: by this point your `BeautifulSoup`-parseable data should be accessible via `city_data[page_id]['text']`, and for output your function should return a `links_index` object again, except now there will be no `'next_page'`, as these lists of links will be complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8:Function(6/6)\n",
    "\n",
    "def collect_all_page_outlinks(city_data):\n",
    "    links_index = {}\n",
    "    \n",
    "    #---your code starts here---\n",
    "            \n",
    "    #---your code stops here---\n",
    "    \n",
    "    return links_index \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(543,\n",
    " [('Armagh (disambiguation)', './Armagh_(disambiguation)'),\n",
    "  ('Scots language', './Scots_language'),\n",
    "  ('Irish language', './Irish_language'),\n",
    "  (\"St. Patrick's Cathedral, Armagh (Roman Catholic)\",\n",
    "   \"./St._Patrick's_Cathedral,_Armagh_(Roman_Catholic)\"),\n",
    "  ('Northern Ireland', './Northern_Ireland'),\n",
    "  ('United Kingdom census, 2011', './United_Kingdom_census,_2011'),\n",
    "  ('Irish grid reference system', './Irish_grid_reference_system'),\n",
    "  ('Belfast', './Belfast'),\n",
    "  ('Local government in Northern Ireland',\n",
    "   './Local_government_in_Northern_Ireland'),\n",
    "  ('Armagh City, Banbridge and Craigavon District Council',\n",
    "   './Armagh_City,_Banbridge_and_Craigavon_District_Council')])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8:SanityCheck\n",
    "\n",
    "outlinks_index = collect_all_page_outlinks(city_data)\n",
    "len(outlinks_index['./Armagh']['links']), outlinks_index['./Armagh']['links'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C9.__ _(2 pts)_ Finally, using the `outlinks_index` from __C8__, determine which city had the _largest_ number of out-links and print the result using the `Inline()` cell, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C9:Inline(2/2)\n",
    "\n",
    "largest_num_links = 0\n",
    "largest_link_city_id = ''\n",
    "\n",
    "#---your code starts here---\n",
    "        \n",
    "#---your code stops here---\n",
    "\n",
    "print('The most out-linked city was ', largest_link_city_id, \n",
    "      ' with ', largest_num_links, ' links.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
